#### 1、IO多路复用：select、poll、epoll的区别（非常重要，几乎必问，回答得越底层越好，要会使用）

##### select:

工作原理：内核调用select传入要监听的文件描述符集合（可读、可写或异常）开始监听，select处于阻塞状态，当有事件发生或设置的等待时间timeout到了就会返回，返回之前自动去除集合中无事件发生的描述符，返回时传出有事件发生的文件描述符集合。但select传出的集合并没有告诉用户集合中包括那几个就绪的文件描述符，需要用户后续进行遍历操作。

select底层采用数组存储

优缺点：

优点：(1) select的可移植性好，可以跨平台；

​			(2) select可设置的监听时间timeout精度更好，可以精确到微妙，而poll毫秒

缺点：(1) select支持的文件描述符数量上限为1024，不能根据用户需求进行更改（除非更改内核代码）；

​			(2) select每次调用时都要将文件描述符集合从用户态拷贝到内核态，开销较大；

​			(3) select返回的就绪文件描述符集合，需要用户遍历监听的所有文件描述符是否在集合中，当监听的文件描述符数量很大时，效率很低。

##### poll:

工作原理：内核调用poll传入要监听的文件描述符数组开始监听，poll处于阻塞状态，当有事件发生或设置的等待时间timeout到了就会返回，返回之前将数组中有事件发生的文件描述符所在的pollfd中的revents设置为返回事件的标识符，返回时传出更新后的数组。与select一样，poll传出的数组也没有告诉用户哪几个文件描述符发生了事件，需要用户遍历查询。

poll底层采用链表存储

优缺点：

优点：(1) poll在文件描述符较大时速度比select更快；

​			(2) poll没有文件描述符连接数限制，可根据用户需求设置最大文件数；

缺点：(1) 存储文件描述符的链表不会保存在内核中，poll每次调用时都要将链表从用户空间拷贝到内核空间，开销很大；

​			(2) poll需要用户遍历监控的所有文件描述符的数组查看是否发生事件，当监听的文件描述符数量很大时，效率很低；

​			(3) poll的工作方式为水平触发(LT)，不支持边沿触发(ET)，效率很低。

##### epoll：

工作原理：内核利用epoll_create创建一个epfd文件描述符，并利用红黑树数据结构将其存储到根节点，在用epoll_ctl从红黑树中添加、修改或移除文件描述符，最后内核调用epoll_wait函数开始监听，epoll处于阻塞状态，当有事件发生或设置的等待时间timeout到了就会返回，返回时传出有事件发生文件描述符的结构体数组。与select、poll不一样的是，epoll直接传出有事件发生的文件描述符数组，不用遍历查询。

epoll底层采用红黑树进行存储

优缺点：

优点：(1) epoll在epoll_wait函数返回时直接传出有事件发生文件描述符的数组，无需重新遍历，效率很高，不会随着文件描述符数量的增加而降低效率；

​			(2) epoll由一组函数实现，epoll_create创建的红黑树保存在内核中，epoll_ctl只需从红黑树中添加、删除或修改节点，无需重复将已有的文件描述符拷贝到内核中，开销较小；

​			(3) epoll没有文件描述符连接数限制，只跟系统内存的大小有关；

​			(4) epoll支持边沿触发，可减少epoll_wait调用次数，提高效率；

缺点：(1) epoll的可移植性没有select好，只能工作在linux环境中；

**对于高并发服务器而言，应该首选epoll多路复用模型；对于要求有良好的跨平台性能而言，应该首选select多路复用模型。**

#### 2、手撕一个最简单的server端服务器（socket、bind、listen、accept这四个API一定要非常熟练）

#### 客户端：socket， connect， 

```c
#include <stdio.h>
#include <arpa/inet.h>
#include <unistd.h>
#include <stdlib.h>
#include <string.h>
#include <sys/select.h>

int main() {
    // 创建socket
    int lfd = socket(PF_INET, SOCK_STREAM, 0);
    struct sockaddr_in saddr;
    saddr.sin_port = htons(9999);
    saddr.sin_family = AF_INET;
    saddr.sin_addr.s_addr = INADDR_ANY;
    
    // 绑定
    bind(lfd, (struct sockaddr*)&saddr, sizeof(saddr));
    
    // 监听
    listen(lfd, 8);
    
    //创建一个fd_set的集合，存放的是需要检测的文件描述符
    fd_set reset, tmp;
   	FD_ZERO(&reset);
    FD_SET(lfd, &reset);
    int maxfd = lfd;
    
    while(1) {
        tmp = reset;
        
        // 调用select系统函数，让内核帮检测哪些文件描述符有数据
        int ret = select(maxfd + 1, &tmp, NULL, NULL, NULL);
        if(ret == -1) {
            perror("select");
            exit(-1);
        } else if(ret == 0) {
            continue;
        } else if(ret > 0) {
            // 说明检测到了有文件描述符的数据对应的缓冲区发生了改变
            if(FD_ISSET(lfd, &tmp)) {
                // 表示有新的客户端连接进来了
                struct sockaddr_in cliaddr;
                int len = sizeof(cliaddr);
                int cfd = accept(lfd, (struct sockaddr*)&cliaddr, &len);
                
                // 将新的文件描述符加入到新的集合中
                FD_SET(cfd, &reset);
                
                // 更新最大的文件描述符
                maxfd = maxfd > cfd ? maxfd : cfd;
            }
            
            for(int i = lfd + 1; i < maxfd; i++) {
                if(FD_ISSET(i, &tmp)) {
                    // 说明这个文件描述符对应的客户端发来了数据
                    char buf[1024] = {0};
                    int len = read(i, buf, sizeof(buf));
                    if(len == -1) {
                        perror("read");
                        exit(-1);
                    } else if(len == 0) {
                        printf("client close...\n");
                        close(i);
                        FD_CLR(i, &reset);
                    } else if(len > 0) {
                        printf("read buf = %s\n", buf);
                        write(i, buf, stelen(buf) + 1);
                    }
                }
            }
        }
    }
    close(lfd);
    return 0;
}
```



#### 3、线程池

线程池是由服务器预先创建的一组子线程，线程池中的线程数量应该和CPU数量差不多。线程池中的所有子线程都运行者相同的代码。当有新的任务到来时，主线程将通过某种方式选择线程池中的某一个子线程来为之服务。

管理一个任务队列，一个线程队列，然后每次取一个任务分配给一个线程去做，循环往复。



避免了线程的重复的创建与释放，实现线程的复用，使效率大大提高



设计步骤：

1.设置一个生产者消费者模型，作为临界资源

2.初始化多个线程，并让其运行起来，从消息队列中取任务执行

3.如果消息队列没有任务，所有线程阻塞

4.当生产者队列来了一个任务，先对消息队列加锁，然后把任务加到队列上，然后使用条件变量，或者信号量取通知阻塞中的线程里处理。

```c++
#ifndef LOCKER_H
#define LOCKER_H

#include <pthread.h>
#include <exception>
#include <semaphore.h>

// 互斥锁类
class locker {
public:
	locker() {
        if(pthread_mutex_init(&m_mutex, NULL) != 0) {
            throw std::exception();
        }
    }
    ~locker() {
        pthread_mutex_destory(&m_mutex);
    }
    bool lock() {
        return pthread_mutex_lock(&m_mutex) == 0;
    }
    bool unlock() {
        return pthread_mutex_unlock(&m_mutex) == 0;
    }
    pthread_mutex_t * get() {
        return &m_mutex;
    }
private:
    pthread_mutex_t m_mutex;
};

// 信号量类
class sem {
public:
    sem() {
        if(sem_init(&m_sem, 0, 0) != 0) {
            throw std::exception();
        }
    }
    sem(int num) {
        if(sem_init(&m_sem, 0 num) != 0) {
            throw std::exception();
        }
    }
    ~sem() {
        sem_destory(&m_sem);
    }
    
    //等待信号量
    bool wait() {
        return sem_wait(&m_sem) == 0;
    }
   	
    //增加信号量
    bool post() {
        return sem_post(&m_sem) == 0;
    }
private:
    sem_t sem;
};


#endif
```



```c++
#ifndef THREADPOLL_H
#define THREADPOLL_H

#include <pthread.h>
#include <list>
#include <exception>
#include <cstdio>
#include "locker.h"

// 线程池类，定义成模版类是为了代码的复现，模版参数T是任务类
template<typename T>
class threadpool {
public:
    threadpool(int thread_number = 8, int max_request = 10000);
    ~threadpoll();
    bool append(T* request);

private:
    static void* worker(void* arg);
    void run();
    
private:
    // 线程的数量
    int thread_number;
    
    // 线程池数组，大小为m_thread_number
    pthread_t * m_threads;
    
    // 请求队列中最多允许的，等待处理的请求数量
    int max_requests;
    
    // 请求队列
    std::list<T*> m_workqueue;
    
    //互斥锁
    locker m_queuelocker;
    
    //信号量用来判断是否有任务需要处理
    sem m_queuestat;
    
    //是否结束线程
    bool m_stop;
};

template<typename T>
threadpool<T>::threadpool(int thread_number, int max_request):m_thread_number(thread_number), m_max_requests(max_request), m_stop(false), m_threads(NULL) {
    if((thread_number <= 0) || (max_request <=0)) {
        throw std::exception();
    }
    m_threads = new pthread_t[m_thread_number];
    if(!m_threads) {
        throw std::exception();
    }
    
    // 创建thread_number个线程，并将他们设置成线程脱离
    for(auto i = 0; i < thread_number; ++i) {
        printf("create the %dth thread\n", i);
        if(pthread_create(m_threads + i, NULL, worker, this) != 0) {
            delete[] m_threads;
            throw std::exception();
        }
        if(pthread_detach(m_threads[i])) {
            delete[] m_threads;
            throw std::exception();
        }
    } 
}

template<typename T>
threadpool<T>::~threadpool() {
    delete[] m_threads;
    m_stop = true;
}

template<typename T>
bool threadpool<T>::append(T* request) {
    m_queuelocker.lock();
    if(m_workqueue.size() > m_max_requests) {
        m_queuelocker.unlock();
        return false;
    }
    
    m_workqueue.push_back(request);
    m_queuelocker.unlock();
    m_queuestat.post();
    return true;
}

template<typename T>
void* threadpool<T>::worker(void * arg) {
    threadpool * pool =(threadpool *)arg;
    pool->run();
    return pool;
}

template<typename T>
void threadpool<T>::run() {
    while(!m_stop) {
        // 从队列中取任务，做任务
        m_queuestat.wait();
        m_queuelocker.lock();
        if(m_workqueue.empty()) {
            m_queuelocker.unlock();
            continue;
        }
        T* request = m_workqueue.front();
        m_workqueue.pop_front();
        m_queuelocker.unlock();
        
        if(!request) {
            continue;
        }
        
        request->process();
    }
}

#endif
```



#### 4、reactor, proactor模式

##### reactor模式

要求主线程(I/O处理单元)只负责监听文件描述符上是否有事件发生，有的话就立即将该事件通知工作线程（逻辑单元），将socket可读可写事件放入请求队列，交给工作线程处理。除此之外，主线程不做任何其他实质性的工作。读写数据，接受新的连接，以及处理客户请求均在工作线程中完成。

**优点**：Reactor实现相对简单，对于大量的IO请求，降低了CPU的负载，对于连接多，但耗时短的处理场景高效；

**缺点**：Reactor处理耗时长的操作会造成文件分发的阻塞，影响到后续事件的处理；

**适用场景**：Reactor适用于同时接收多个服务请求，并且依次同步的处理它们的事件驱动程序。

##### proactor模式

proactor模式将所有I/O操作都交给主线程和内核来处理（进行读、写），工作线程仅仅负责业务逻辑。

**优点**：Proactor在理论上性能更高，能够处理耗时长的并发场景。

**缺点**：Proactor实现逻辑复杂，依赖操作系统对异步的支持。

**适用场景**：异步接收和同时处理多个服务请求的事件驱动程序。



#### 5、边沿触发与水平触发的区别

使用epoll多路复用编程时，会用epoll_wait阻塞等待事件的发生，对应有边沿触发和水平触发两种工作方式。

##### 水平触发：只要缓冲区有数据，epoll_wait就会一直被触发，直到缓冲区为空。

优点：保证了数据的完整输出

缺点：当数据较大时，需要不断从用户态和内核态 切换，消耗了大量的系统资源，影响服务器性能。

##### 边沿触发：只要所监听的事件状态改变或者有事件发生时，epoll_wait才会被触发；

边沿触发通常与非阻塞IO一起使用，其工作模式为：epoll_wait触发一次，在while(1)循环内非阻塞IO读取数据，直到缓冲区数据为空(保证了数据的完整性)，内核才会继续调用epoll_wait等待时间发生。

边沿触发+非阻塞IO优缺点：

优点：每次epoll_wait只用触发一次，就可以读取缓冲区的所有数据，工作效率高，大大提升了服务器性能。

缺点：数据量很小时，至少需要调用两次非阻塞IO函数，而边沿触发只用调用一次。



#### 6、非阻塞IO与阻塞IO区别

阻塞IO：调用者调用了某个函数，等待这个函数返回，期间什么都不做，不停的去检查这个函数有没有返回，必须等这个函数返回才能进行下一步动作。

非阻塞IO：每隔一段时间就去检测IO事件是否就绪。没有就绪就可以做其他事。非阻塞IO执行系统调用总是立刻返回，不管事件是否已经发生。

#### 生产者消费者模型

```c++
#include<pthread.h>
pthread_mutex_t mutex;
pthread_cond_t cond;

queue<T> WorkQueue;

void *producer(void* arg){
	while(1) {
        pthread_mutex_lock(&mutex);
        WorkQuere.push(T work);
        
        pthread_cond_signal(&cond);
        pthread_mutex_unlock(&mutex);
    }
    return NULL;
}

void *customer(void* arg) {
    while(1) {
        pthread_mutex_lock(&mutex);
        if(!WorkQuere.empty()) {
			T work = WorkQueue.front();
            WorkQueue.pop();
            work->do_request();
     		pthread_mutex_unlock(&mutex);
        } else{
            pthread_cond_wait(&cond, &mutex);
            pthread_mutex_unlock(&mutex);
        }
    }
    return NULL;
}

int main(){
    
    pthread_mutex_init(&mutex, NULL);
    pthread_cond_init(&cond, NULL);
    
    
    
    pthread_t ptids[5], ctids[5];
    for(int i = 0; i < 5; i++) {
		pthread_create(&ptid[i], NULL, producer, NULL;
        pthread_create(&ctid[i], NULL, customer, NULL);
    }
    
   	for(int i = 0; i < 5; i++) {
		pthread_detach(ptid[i]);
        pthread_detach(ctid[i]);
    }
                       
    pthread_mutex_destory(&mutex);
    pthread_cond_destory(&cond);
                       
    pthread_exit(NULL);
                       
    
    
	return 0;
}
```

#### 零拷贝

调用如sendfile()，操作系统由用户态切换到内核态，通过DMA引擎将数据从磁盘拷贝到内核态里的输入socket缓冲区，再拷贝到目标socket缓冲区，然后将数据拷贝到协议引擎，调用sendfile()结束，操作系统从内核态切换到用户态空间。

从操作系统的角度来看，这就是零拷贝。节省了CPU周期，减少了用户和内核态切换

#### socket通信发生了错误

查看端口是否被监听，查看防火墙，最后在查看代码是否有问题

#### 项目问题

创建好线程池后，连接网页没有反应：把socket的参数从本地字节序改为网络字节序。

url不全。

**线程池+非阻塞socket+epoll+事件处理的并发模型proactor**

**状态机解析HTTP请求**
